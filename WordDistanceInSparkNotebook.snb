{"metadata":{"name":"Untitled1","user_save_timestamp":"1970-01-01T01:00:00.000Z","auto_save_timestamp":"1970-01-01T01:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"customLocalRepo":null,"customRepos":null,"customDeps":null,"customImports":null,"customSparkConf":{"spark.app.name":"Notebook","spark.master":"local[8]","spark.executor.memory":"1G"}},"cells":[{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"import org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.apache.spark.rdd.RDD","outputs":[{"name":"stdout","output_type":"stream","text":"import org.apache.spark.SparkContext\nimport org.apache.spark.SparkContext._\nimport org.apache.spark.SparkConf\nimport org.apache.spark.rdd.RDD\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":9}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"  \n  type Hit = (Long, String) // (index, the significant word found at index)\n  type SameWordInterval = ((Long,Long),String) // ( (startIdx, endIdx), significant word)\n","outputs":[{"name":"stdout","output_type":"stream","text":"defined type alias Hit\ndefined type alias SameWordInterval\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":10}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":" /**\n   * Finds the distance between given words in the text, defined as \n   * minimum amount of other words in between occurrences of the given words.\n   * \n   * Distance between a word and itself is 0.\n   * If one of the words does not occur in the text, distance is None\n   *  \n   * @param textFilePath\n   * @param word1\n   * @param word2\n   * @param sc sparkContext\n   * @return\n   */\n  def wordDistance(textFilePath:String, word1:String, word2:String)(implicit sc:SparkContext):Option[Long] = {\n    if (word1 == word2) return Some(0)\n    if (word1==null || word2==null) return None\n    \n    // need access to a cluster to tweak this experimentally, so this is just a guess. \n    // I don't know whether even to fiddle with this at all or just take the defaultParallelism.\n    implicit val nbPartitions = sc.defaultParallelism * 2\n    \n    // would this normally have to be broadcasted to every node then?\n    val lines:RDD[String] = sc.textFile(textFilePath, nbPartitions)\n    \n    // look at text file as sequence of words\n    val words = asFlatWordList(lines)\n    \n    // we filter out all words that are not word1 or word2\n    val orderedHits:RDD[Hit] = findHitsInOrder(words, word1, word2)\n    println(\"\\n ***** Nb hits: \"+orderedHits.count()+\" .\\n\")\n    \n    val smallestDistance:Option[Long] = findSmallestDistanceBetweenDifferentWordHits(orderedHits)\n    smallestDistance\n  }\n\n  def asFlatWordList(lines:RDD[String]):RDD[String] = lines.flatMap { l => l.split(\" \") }\n\n  def findHitsInOrder(words:RDD[String], word1:String, word2:String)(implicit nbPartitions:Int):RDD[Hit] = {\n    // this part should be embarrassingly parallel, as we like it\n    val hits = words.zipWithIndex().filter(p=> (p._1 == word1 || p._1 == word2)).map(p => p.swap)\n    // the sort destroys the parallelism, but is necessary for avoiding unnecessary comparisons\n    // I'm assuming here that spark implements this sort well, in order to make this still an overall win.\n    val sorted = hits.sortByKey(ascending=true, numPartitions = nbPartitions)\n    sorted\n  }\n  \n  def findSmallestDistanceBetweenDifferentWordHits(orderedHits:RDD[Hit]): Option[Long] = {\n    // println(\"\"\"***** \"\"\"+orderedHits.top(100).toList.mkString(\", \"))\n    \n    // Our input data is like a pseudo-timeseries, with word indices as time axis, \n    // and only two possible values: word1 or word2.\n    // At this point we can still have repetitions of the same word.\n    // But a smallest distance can only happen when the signal transitions between hits of word1 and word2 or other other.\n    // Our interface allows us to not care about the indices of the transition itself, \n    // only about the index-distance  between the words forming the transition\n    // Since we don't care about the transitions, automatic/manual stream fusion should in theory \n    // allow to get rid of the transition-finding as a separate step.\n    // But given the time limitation, and because this kind of tweaking might be \n    // much less important than other changes, I'm writing it here as the conceptual sequence of \n    // first finding transitions, and then finding the smallest one.\n\n    // I wish I knew spark well enough already to be able to split the RDD of Hits in partitions\n    // which do not break a transition across partitions, in order to avoid a synchronization step.\n    // I immediately see two approaches that would work: \n    // 1) If I could include the hit which happens to be part of a signal transition in both partitions, I'd be sure\n    // that no transition is broken up across partitions, and then the partitions \n    // would only need to communicate back their winning candidate. seems fail-safe\n    // 2) If I could align the partition boundaries to only fall in the middle of hits of the same word\n    // we also would not have a broken transition.\n    // But if we only do option 2, our number of partitions would depend strongly on the input text.\n    \n    // Without a solution for avoiding the synchronization, I don't yet have a sense for how much of an improvement \n    // it gives to do this part of the computation on the cluster instead of in the driver.\n    // For a non-pathological case, the filtering all words to just our word1 and word2 should already be \n    // a huge shrink in dataset size.\n    \n    // but since there was a big warning against the trivial \"make it run on spark\" \n    // implementation of immediately collecting, I'll try to write it in a spark-compatible style.\n    // \n    // Stylewise, I'm unhappy because it becomes a bit car/cdr soup though, \n    // which could be solved by defining case classes.\n\n    // I can't find a spark foldLeft which lets me fold Hits into SameWordIntervals.\n    // A verbose workaround but pretty foolproof solution would be to create a trait as union type.\n    // In this case I took the \"wrap everything as a list\" approach. Pretty ugly.\n    // I'm not sure whether defining a custom accumulator would help for making it less ugly \n    // and for making it more performant.\n     \n    val reverseOrderedSameWordIntervals:List[SameWordInterval] = convertHitsToSameWordIntervals(orderedHits)\n    // the fact that I do a fold which is an action instead of a transform, \n    // is bumping me out of RDDs into regular scala-land at this point anyway.\n    // At this point we have had another huge data shrink from skipping consecutive hits of the same word,\n    // and it is already late, so I'm going to do the last part in pure scala\n    \n    // println(\"\"\"***** \"\"\"+reverseOrderedSameWordIntervals.take(100).toList.mkString(\", \"))p\n    \n    println(\"\\n***** Nb same word intervals: \"+reverseOrderedSameWordIntervals.size+\" .\\n\")\n    \n    // if one of the words does not exist, there will only be one element in this list\n    if (reverseOrderedSameWordIntervals.size == 1) {\n      None \n    } else {\n      Some(findMinimumDistanceBetweenAlternatingIntervals(reverseOrderedSameWordIntervals))\n    }\n  }\n   \n  def convertHitsToSameWordIntervals(orderedHits:RDD[Hit]):List[SameWordInterval] = {\n    // a hit becomes an interval with same start as end\n    orderedHits.map(hit => List(((hit._1,hit._1), hit._2))).\n      fold(List[SameWordInterval]())(foldIntoExistingSameWordIntervals _)\n  }\n  \n  def foldIntoExistingSameWordIntervals(acc:List[SameWordInterval], extra:List[SameWordInterval]): List[SameWordInterval] = {\n    // warning: car/cdr soup ahead. Which is why this part is unittested and gets a post-condition.\n    def wordAt(swi: SameWordInterval):String = swi._2\n    \n    //println(\"* Adding \"+extra +\" into \"+acc)\n    \n    var shouldMerge = false\n    val result = if (acc.isEmpty) { \n      extra\n    } else {\n        val endOfAcc = acc.head\n        val beginOfExtra = extra.last\n        shouldMerge = wordAt(endOfAcc)==wordAt(beginOfExtra)\n        if (shouldMerge) {\n          val mergedLink = ((acc.head._1._1, extra.last._1._2), wordAt(endOfAcc))\n          extra.dropRight(1):::(mergedLink::acc.tail)\n        } else {\n          extra:::acc\n        }\n    }\n\n    val expectedSize = acc.size + extra.size - (if (shouldMerge) 1 else 0)\n    if (result.size != expectedSize) throw new Exception(\"Programming error in merging \"+extra +\" into acc \"+acc+\": wrong size of result \"+result)\n    result\n  }\n  \n  def findMinimumDistanceBetweenAlternatingIntervals(reverseOrderedSameWordIntervals:List[SameWordInterval]):Long = {\n    // in pure scala I get sliding, in spark this seems to be part of the MLLib. I have not investigated further.\n    val distances = reverseOrderedSameWordIntervals.sliding(2).map {\n        case laterInterval::precedingInterval::Nil => calcDist(precedingInterval, laterInterval)\n      }\n    distances.min\n  }\n\n  def calcDist(first:SameWordInterval, second:SameWordInterval):Long = calcDist(first._1._2, second._1._1)\n  \n  def calcDist(first:Long, last:Long):Long = last-first-1\n  \n ","outputs":[{"name":"stdout","output_type":"stream","text":"<console>:210: warning: match may not be exhaustive.\nIt would fail on the following input: List(_, _, _)\n    val distances = reverseOrderedSameWordIntervals.sliding(2).map {\n                                                                   ^\nwordDistance: (textFilePath: String, word1: String, word2: String)(implicit sc: org.apache.spark.SparkContext)Option[Long]\nasFlatWordList: (lines: org.apache.spark.rdd.RDD[String])org.apache.spark.rdd.RDD[String]\nfindHitsInOrder: (words: org.apache.spark.rdd.RDD[String], word1: String, word2: String)(implicit nbPartitions: Int)org.apache.spark.rdd.RDD[(Long, String)]\nfindSmallestDistanceBetweenDifferentWordHits: (orderedHits: org.apache.spark.rdd.RDD[(Long, String)])Option[Long]\nconvertHitsToSameWordIntervals: (orderedHits: org.apache.spark.rdd.RDD[(Long, String)])List[((Long, Long), String)]\nfoldIntoExistingSameWordIntervals: (acc: List[((Long, Long), String)], extra: List[((Long, Long), String)])List[((Long, Long), String)]\nfindMinimumDistanceBetweenAlternatingIntervals: (reverseOrder..."},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":11}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"  def testFoldHandlesFirstHit() {\n    val expected = List(((1,4),\"Foo\"))\n    val actual = foldIntoExistingSameWordIntervals(List(), List(((1,4),\"Foo\")))\n    if (expected != actual) throw new Exception(actual.toString())\n  }\n  \n  def testFoldHandlesSequenceOfSameWords() {\n    val expected = List( ((3,7),\"Bar\"), ((1,1),\"Foo\") )\n    val actual = foldIntoExistingSameWordIntervals(List(((3,3),\"Bar\"),((1,1),\"Foo\")), List(((7,7),\"Bar\")))\n    if (expected != actual) throw new Exception(actual.toString())\n  }\n  \n  def testFoldHandlesSequenceOfDifferentWords() {\n    val expected = List( ((3,3),\"Bar\"),  ((1,1),\"Foo\") )\n    val actual = foldIntoExistingSameWordIntervals(List( ((1,1),\"Foo\") ), List(((3,3),\"Bar\")))\n    if (expected != actual) throw new Exception(actual.toString())\n  }\n  \n  def testFoldHandlesIntermediateMergingCase() {\n    val expected = List( ((10,10),\"Foo\"), ((3,7),\"Bar\"), ((1,1),\"Foo\") )\n    val actual = foldIntoExistingSameWordIntervals(acc=List( ((3,3), \"Bar\"), ((1,1),\"Foo\")), extra=List( ((10,10),\"Foo\"), ((5,7),\"Bar\")))\n    if (expected != actual) throw new Exception(actual.toString())\n  }","outputs":[{"name":"stdout","output_type":"stream","text":"testFoldHandlesFirstHit: ()Unit\ntestFoldHandlesSequenceOfSameWords: ()Unit\ntestFoldHandlesSequenceOfDifferentWords: ()Unit\ntestFoldHandlesIntermediateMergingCase: ()Unit\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":12}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"    testFoldHandlesFirstHit()\n    testFoldHandlesSequenceOfSameWords()\n    testFoldHandlesSequenceOfDifferentWords()\n    testFoldHandlesIntermediateMergingCase()","outputs":[{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":13}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"// practical test 2 runs on the well-tokenized shakespeare corpus, \n    // which is downloadable at http://norvig.com/ngrams/shakespeare.txt\n    val testFileShakespeare = \"shakespeare.txt\"\n    val minRomeoToJuliet = wordDistance(testFileShakespeare, \"Romeo\", \"Juliet\")(sparkContext)\n    println(\"\\n***** distance between Romeo and Juliet : \"+minRomeoToJuliet+\"\\n\")\n    // excerpt from shakespeare.txt: Is father , mother , Tybalt , Romeo , Juliet ,\n    if (Some(1L) != minRomeoToJuliet) {\n      throw new Exception(\"\"\"Distance between \"Romeo\" and \"Juliet found to be \"\"\"+minRomeoToJuliet+\n          \"\"\". However, this was working on Jelle's machine on June 12th.\"\"\")\n    }","outputs":[{"name":"stdout","output_type":"stream","text":"\n ***** Nb hits: 158 .\n\n\n***** Nb same word intervals: 51 .\n\n\n***** distance between Romeo and Juliet : Some(-819071)\n\njava.lang.Exception: Distance between \"Romeo\" and \"Juliet found to be Some(-819071). However, this was working on Jelle's machine on June 12th.\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:71)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:81)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:83)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:85)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:87)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:89)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:91)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:93)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:95)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:97)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:99)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:101)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:103)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:105)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:107)\n\tat $iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:109)\n\tat $iwC$$iwC$$iwC$$iwC.<init>(<console>:111)\n\tat $iwC$$iwC$$iwC.<init>(<console>:113)\n\tat $iwC$$iwC.<init>(<console>:115)\n\tat $iwC.<init>(<console>:117)\n\tat <init>(<console>:119)\n\tat .<init>(<console>:123)\n\tat .<clinit>(<console>)\n\tat .<init>(<console>:7)\n\tat .<clinit>(<console>)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:497)\n\tat org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)\n\tat org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1338)\n\tat org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)\n\tat org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)\n\tat notebook.kernel.Repl$$anonfun$3.apply(Repl.scala:168)\n\tat notebook.kernel.Repl$$anonfun$3.apply(Repl.scala:168)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:57)\n\tat scala.Console$.withOut(Console.scala:126)\n\tat notebook.kernel.Repl.evaluate(Repl.scala:167)\n\tat notebook.client.ReplCalculator$$anonfun$10$$anon$1$$anonfun$22.apply(ReplCalculator.scala:321)\n\tat notebook.client.ReplCalculator$$anonfun$10$$anon$1$$anonfun$22.apply(ReplCalculator.scala:318)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\n\tat scala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\n\tat akka.dispatch.TaskInvocation.run(AbstractDispatcher.scala:41)\n\tat akka.dispatch.ForkJoinExecutorConfigurator$AkkaForkJoinTask.exec(AbstractDispatcher.scala:393)\n\tat scala.concurrent.forkjoin.ForkJoinTask.doExec(ForkJoinTask.java:260)\n\tat scala.concurrent.forkjoin.ForkJoinPool$WorkQueue.runTask(ForkJoinPool.java:1339)\n\tat scala.concurrent.forkjoin.ForkJoinPool.runWorker(ForkJoinPool.java:1979)\n\tat scala.concurrent.forkjoin.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:107)\n\n"}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":false},"cell_type":"code","source":"println(\"This is very puzzling. In the notebook it collapses the 158 Hits down into 51 SameWordIntervals, and then goes completely haywire. When calling sbt run, it finds 50 SameWordIntervals and is ok.\")\n","outputs":[{"name":"stdout","output_type":"stream","text":"This is very puzzling. In the notebook it collapses the 158 Hits down into 51 SameWordIntervals, and then goes completely haywire. When calling sbt run, it finds 50 SameWordIntervals and is ok.\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":16}]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"","outputs":[]},{"metadata":{"trusted":true,"input_collapsed":false,"collapsed":true},"cell_type":"code","source":"","outputs":[]}],"nbformat":4}